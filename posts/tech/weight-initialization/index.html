<!DOCTYPE html>
<html lang='en' class="js csstransforms3d">
  <head>
    <title>Thuyen&#39;s corner</title>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta property="og:title" content="Weight initialization - impact on layer distribution" />
<meta property="og:description" content="This post covers some experiments to demonstrate the impact of weight initialization on the distribution of activations on each layer in neural network, especially the very last layers." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://trinhngocthuyen.com/posts/tech/weight-initialization/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2017-07-25T00:00:00+00:00" />
<meta property="article:modified_time" content="2017-07-25T00:00:00+00:00" />


<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/default.min.css">
<link rel="stylesheet" href="/sass/main.48eba9c64b010727a622acef1a23debdea4ad9239e3037dcbc1962643a25b193.css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
<script src="https://kit.fontawesome.com/b0896cba62.js" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.7.1/jquery.min.js"></script>

<script async src="https://www.googletagmanager.com/gtag/js?id=G-VWZ70X3N46"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-VWZ70X3N46', { 'anonymize_ip': false });
}
</script>

  </head>
  <body class="" data-url="/posts/tech/weight-initialization/">
    <nav role="navigation">
  <header class="banner">
    <h1 id="logo">
       <a href="/"><span class="first">üè† THUYEN's</span> <span class="second">corner</span></a>
    </h1>
    <div id="menu-toggle" class="menu-toggle">
      <span class='open'><svg class='icon' viewbox='0 0 24 24' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' aria-hidden='true'>
  <line x1="3" y1="12" x2="21" y2="12" />
  <line x1="3" y1="6" x2="21" y2="6" />
  <line x1="3" y1="18" x2="21" y2="18" />
</svg>
</span>
      <span class='close'><svg class='icon' viewbox='0 0 24 24' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' aria-hidden='true'>
  <line x1="18" y1="6" x2="6" y2="18" />
  <line x1="6" y1="6" x2="18" y2="18" />
</svg>
</span>
    </div>
  </header>
  <ul class="menu-items">
    <li class="menu-item"><a href="/posts/tech">Tech Blog</a></li>
    <li class="menu-item"><a href="/posts/misc">T·∫°p B√∫t</a></li>
    <li class="menu-item"><a href="/projects">Projects</a></li>
    <li class="menu-item"><a href="/about">About</a></li>
  </ul>
</nav>

<script src="/scripts/menu.js"></script>
    <main role="main">
<div class="page-wrapper">
<article class="page">
  <header>
  
  <h1>Weight initialization - impact on layer distribution</h1>
  
  
  <div class="metadata">
  <span>2017, Jul 25</span>
</div>
  
</header>
  <p>In this post, I will carry out some experiments to demonstrate the impact of weight initialization on the distribution of activations on each layer in neural network, especially the very last layers. This was mentioned by Andrej in <a href="https://www.youtube.com/watch?v=hd_KFJ5ktUc&amp;index=6&amp;list=PLkt2uSq6rBVctENoVBg1TpCC7OQi31AlC">cs231n/lec6</a> as a motivation paving the way for batch normalization.</p>
<p>I will try to keep things simple with intuition. Mathematics will not be richly convered.</p>
<h2 id="1-covariate-shift">1. Covariate Shift</h2>
<h3 id="11-what-is-covariate-shift">1.1. What is Covariate Shift?</h3>
<p><em>Covariate shift</em> refers to changes in the distribution of input variables. This shift is usually addressed as a problem causing poor performance in training neural networks.</p>
<p>Note: In the context of neural networks, a layer takes the activations of the previous layer as its input. Therefore, it is also equivalent if we investigate the distribution of activations of a layer.</p>
<h3 id="12-what-does-it-look-like">1.2 .What Does it Look Like?</h3>
<p>Consider a neural networks with 20 fully connected layers.</p>
<ul>
<li><em>Input shape</em>: 49000x3072 (reshape from 49000 CIFAR images of size 32x32) and was whitened (zero-centered &amp; normalized).</li>
<li><em>Hidden layers</em>: 19 layers of shape (100, 100)</li>
<li><em>Activation function</em>: ReLU.</li>
<li><em>Biases</em>: all zeros.</li>
<li><em>Weights</em>: normal distribution, scaled by factor 0.1.</li>
</ul>
<p>The distributions of layer ouputs are illustrated in the figure below. We can apparently see that the variance constantly decreases in deeper layers.</p>
<p><img src="/images/notebooks/weight_initialization_5_0.png" width="600px"/>
<figcaption><em>Fig 1.</em> Distribution of activations in layer 1, 3, 5, 7.</figcaption></p>
<h3 id="13-impact-of-covariate-shift">1.3. Impact of Covariate Shift?</h3>
<p>When training a network, if the activations fall in a narrow range like above, the gradient on those activations could be relatively small, leading to <strong>gradient saturation/vanish</strong>. Training gets stuck as a result. We sometimes face the same problem with dispersing distribution.</p>
<h2 id="2-param-initialization">2. Param Initialization</h2>
<p>Param initialization is a primary cause of covariate shift. We will focus on weight initialization, and let biases be all zeros.</p>
<p>An initialization strategy is considered good if it maintains the same mean and variance (or standard deviation) throughout layers. Since the input data was whitened, we expect mean around zero (+/-1), std around 1 (+/-0.5).</p>
<ul>
<li>$\mu = 0.3, \sigma = 0.7$  ‚ü∂ accepted ‚úÖ</li>
<li>$\mu = 0.3, \sigma = 0.003$ ‚ü∂ NOT accepted ‚ùå</li>
<li>$\mu = 15, \sigma = 320$ ‚ü∂ NOT accepted ‚ùå</li>
</ul>
<h3 id="21-setup">2.1. Setup</h3>
<p>Some setup code for experiments:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">relu</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">X_flat</span><span class="p">,</span> <span class="n">hid_dims</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">biases</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">relu</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    Feed forward.
</span></span></span><span class="line"><span class="cl"><span class="s2">     - X_flat: shape: N x D, where D = d1 x ... x dk.
</span></span></span><span class="line"><span class="cl"><span class="s2">     - hid_dims: 1D array.
</span></span></span><span class="line"><span class="cl"><span class="s2">     - weights: array of weights: W1, W2, ...
</span></span></span><span class="line"><span class="cl"><span class="s2">     - biases: array of biases: b1, b2, ...
</span></span></span><span class="line"><span class="cl"><span class="s2">     - activation: activation on each layer. None if no activation.
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">biases</span><span class="p">),</span> <span class="s1">&#39;weights and biases must have the same length&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">hid_dims</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">weights</span><span class="p">),</span> <span class="s1">&#39;len(weights) must equal len(hid_dims) + 1&#39;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">all_layers</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">    <span class="n">out</span> <span class="o">=</span> <span class="n">X_flat</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">hid_dims</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">weights</span><span class="p">[</span><span class="n">l</span><span class="p">])</span> <span class="o">+</span> <span class="n">biases</span><span class="p">[</span><span class="n">l</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">activation</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">out</span> <span class="o">=</span> <span class="n">activation</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">all_layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="n">all_layers</span>
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">visualize_distribution</span><span class="p">(</span><span class="n">all_activations</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">n_layers</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">all_activations</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">activations_mean</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">asscalar</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">activations</span><span class="p">))</span> <span class="k">for</span> <span class="n">activations</span> <span class="ow">in</span> <span class="n">all_activations</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">activations_std</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">asscalar</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">activations</span><span class="p">))</span> <span class="k">for</span> <span class="n">activations</span> <span class="ow">in</span> <span class="n">all_activations</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">xs</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_layers</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">_</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">,</span> <span class="n">ax3</span><span class="p">,</span> <span class="n">ax4</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">ax1</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">activations_mean</span><span class="p">,</span> <span class="s1">&#39;-o&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;#0067a7&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">ax1</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;means by layer&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">ax1</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;layer&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">ax2</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">activations_std</span><span class="p">,</span> <span class="s1">&#39;-o&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">ax2</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;std by layer&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">ax2</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;layer&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">ax3</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">all_activations</span><span class="p">[</span><span class="mi">14</span><span class="p">]</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">ax3</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Distribution of layer 15&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">ax4</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">all_activations</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">ax4</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Distribution of the layer </span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">n_layers</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Print mean and std of last 5 layer</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_layers</span><span class="p">)[</span><span class="o">-</span><span class="mi">5</span><span class="p">:]:</span>
</span></span><span class="line"><span class="cl">        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Layer </span><span class="si">%2d</span><span class="s1">. mean: </span><span class="si">%f</span><span class="se">\t</span><span class="s1">std: </span><span class="si">%f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">l</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">activations_mean</span><span class="p">[</span><span class="n">l</span><span class="p">],</span> <span class="n">activations_std</span><span class="p">[</span><span class="n">l</span><span class="p">]))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">initialize_params</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">hid_dims</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                      <span class="n">weight_initializer</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                      <span class="n">bias_initializer</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">dims</span> <span class="o">=</span> <span class="p">[</span><span class="n">in_dim</span><span class="p">]</span> <span class="o">+</span> <span class="n">hid_dims</span> <span class="o">+</span> <span class="p">[</span><span class="n">out_dim</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">weights</span><span class="p">,</span> <span class="n">biases</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">hid_dims</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">w</span> <span class="o">=</span> <span class="n">weight_initializer</span><span class="p">(</span><span class="n">dims</span><span class="p">[</span><span class="n">l</span><span class="p">],</span> <span class="n">dims</span><span class="p">[</span><span class="n">l</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">        <span class="n">b</span> <span class="o">=</span> <span class="n">bias_initializer</span><span class="p">(</span><span class="n">dims</span><span class="p">[</span><span class="n">l</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">        <span class="n">weights</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">biases</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">weights</span><span class="p">,</span> <span class="n">biases</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">examine_distribution</span><span class="p">(</span><span class="n">weight_initializer</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">relu</span><span class="p">,</span> <span class="n">n_layers</span><span class="o">=</span><span class="mi">20</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">in_dim</span><span class="p">,</span> <span class="n">out_dim</span> <span class="o">=</span> <span class="mi">32</span><span class="o">*</span><span class="mi">32</span><span class="o">*</span><span class="mi">3</span><span class="p">,</span> <span class="mi">10</span>
</span></span><span class="line"><span class="cl">    <span class="n">hid_dims</span> <span class="o">=</span> <span class="p">[</span><span class="mi">100</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_layers</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">weights</span><span class="p">,</span> <span class="n">biases</span> <span class="o">=</span> <span class="n">initialize_params</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">hid_dims</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">weight_initializer</span><span class="o">=</span><span class="n">weight_initializer</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">_</span><span class="p">,</span> <span class="n">all_activations</span> <span class="o">=</span> <span class="n">forward</span><span class="p">(</span><span class="n">X_train_normalized</span><span class="p">,</span> <span class="n">hid_dims</span><span class="o">=</span><span class="n">hid_dims</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">        <span class="n">weights</span><span class="o">=</span><span class="n">weights</span><span class="p">,</span> <span class="n">biases</span><span class="o">=</span><span class="n">biases</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">visualize_distribution</span><span class="p">(</span><span class="n">all_activations</span><span class="p">)</span>
</span></span></code></pre></div><p>Now, lets examine some initialization schemes&hellip;</p>
<h3 id="22-trivial-initialization">2.2. Trivial Initialization</h3>
<p>The common way is using normal randomization with a scale. This scale should equal the standard deviation of the random numbers.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">trivial_initializer</span><span class="p">(</span><span class="n">d1</span><span class="p">,</span> <span class="n">d2</span><span class="p">,</span> <span class="n">weight_scale</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">weight_scale</span><span class="p">,</span> <span class="p">(</span><span class="n">d1</span><span class="p">,</span> <span class="n">d2</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">examine_distribution</span><span class="p">(</span><span class="n">trivial_initializer</span><span class="p">)</span>
</span></span></code></pre></div><p><img src="/images/notebooks/weight_initialization_6_0.png" width="600px"/>
<figcaption><em>Fig 2.</em> mean and std by layer. Initialized by Gaussian $\mu=0, \sigma^2=0.1$.</figcaption></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">Layer 16. mean: 0.009006    std: 0.016165
</span></span><span class="line"><span class="cl">Layer 17. mean: 0.005849    std: 0.009699
</span></span><span class="line"><span class="cl">Layer 18. mean: 0.004207    std: 0.007308
</span></span><span class="line"><span class="cl">Layer 19. mean: 0.002917    std: 0.004961
</span></span><span class="line"><span class="cl">Layer 20. mean: 0.005232    std: 0.006176
</span></span></code></pre></div><p>‚ü∂ Not accepted ‚ùå</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">examine_distribution</span><span class="p">(</span><span class="k">lambda</span> <span class="n">d1</span><span class="p">,</span> <span class="n">d2</span><span class="p">:</span> <span class="n">trivial_initializer</span><span class="p">(</span><span class="n">d1</span><span class="p">,</span> <span class="n">d2</span><span class="p">,</span> <span class="n">weight_scale</span><span class="o">=</span><span class="mf">0.2</span><span class="p">))</span>
</span></span></code></pre></div><p><img src="/images/notebooks/weight_initialization_7_0.png" width="600px"/>
<figcaption><em>Fig 3.</em> mean and std by layer. Initialized by Gaussian $\mu=0, \sigma^2=0.2$.</figcaption></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">Layer 16. mean: 825.965548	std: 1394.566496
</span></span><span class="line"><span class="cl">Layer 17. mean: 1491.038728	std: 2237.786857
</span></span><span class="line"><span class="cl">Layer 18. mean: 2149.680649	std: 3300.200497
</span></span><span class="line"><span class="cl">Layer 19. mean: 3008.496905	std: 4961.155125
</span></span><span class="line"><span class="cl">Layer 20. mean: 2845.114905	std: 4381.190124
</span></span></code></pre></div><p>‚ü∂ Not accepted ‚ùå</p>
<p>Notice that after changing weight scale from 0.1 to 0.2, how output values are distributed changed radically. Quite sensitive to weight scale, huh?</p>
<h3 id="23-xavier-initialization-glorot-initialization">2.3. Xavier Initialization (Glorot Initialization)</h3>
<p>This scheme is based on the observation that:</p>
<p>$$\begin{equation}
var(s) = var(\sum_i^N{w_i.x_i}) = &hellip; = N.var(w).var(x)
\end{equation}$$</p>
<p>In order to keep the same variance, we need: $var(w) = 1/N$. So, the weight scale should be $\sqrt{1/N}$</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">xavier_initializer_1</span><span class="p">(</span><span class="n">d1</span><span class="p">,</span> <span class="n">d2</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">d1</span><span class="p">,</span> <span class="n">d2</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="n">d1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">examine_distribution</span><span class="p">(</span><span class="n">xavier_initializer_1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</span></span></code></pre></div><p><img src="/images/notebooks/weight_initialization_8_0.png" width="600px"/>
<figcaption><em>Fig 4.</em> mean and std by layer. Initialized using xavier initialization, with no activation.</figcaption></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">Layer 16. mean: 0.000000	std: 0.742681
</span></span><span class="line"><span class="cl">Layer 17. mean: -0.000000	std: 0.731260
</span></span><span class="line"><span class="cl">Layer 18. mean: -0.000000	std: 0.718502
</span></span><span class="line"><span class="cl">Layer 19. mean: 0.000000	std: 0.724088
</span></span><span class="line"><span class="cl">Layer 20. mean: -0.000000	std: 0.740663
</span></span></code></pre></div><p>‚ü∂ Accepted ‚úÖ</p>
<p>We can see that xavier initialization gives a really nice distribution: <em>gaussian shape, reasonable mean and std</em>. However, this initialization strategy seems not to work well with non-linear activation. If we use ReLU as the activation function, we got a poor distribution.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">examine_distribution</span><span class="p">(</span><span class="n">xavier_initializer_1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">relu</span><span class="p">)</span>
</span></span></code></pre></div><p><img src="/images/notebooks/weight_initialization_9_0.png" width="600px"/>
<figcaption><em>Fig 5.</em> mean and std by layer. With relu activation, normal distribution, $var(w) = 1/N$.</figcaption></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">Layer 16. mean: 0.002048	std: 0.003093
</span></span><span class="line"><span class="cl">Layer 17. mean: 0.001338	std: 0.002246
</span></span><span class="line"><span class="cl">Layer 18. mean: 0.000834	std: 0.001481
</span></span><span class="line"><span class="cl">Layer 19. mean: 0.000434	std: 0.000776
</span></span><span class="line"><span class="cl">Layer 20. mean: 0.000440	std: 0.000590
</span></span></code></pre></div><p>‚ü∂ Not accepted ‚ùå</p>
<p>A recent paper by <a href="https://arxiv.org/pdf/1502.01852v1.pdf">He et al</a> dove into the analysis with ReLU activation and finally reached the conclusion that the variance of weights should be $2/N$. This is often used in practice.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">xavier_initializer_2</span><span class="p">(</span><span class="n">d1</span><span class="p">,</span> <span class="n">d2</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">d1</span><span class="p">,</span> <span class="n">d2</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">2.0</span> <span class="o">/</span> <span class="n">d1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">examine_distribution</span><span class="p">(</span><span class="n">xavier_initializer_2</span><span class="p">)</span>
</span></span></code></pre></div><p><img src="/images/notebooks/weight_initialization_10_0.png" width="600px"/>
<figcaption><em>Fig 6.</em> mean and std by layer. With relu activation, normal distribution, $var(w) = 2/N$.</figcaption></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">Layer 16. mean: 0.605864	std: 0.924794
</span></span><span class="line"><span class="cl">Layer 17. mean: 0.569898	std: 0.860651
</span></span><span class="line"><span class="cl">Layer 18. mean: 0.530719	std: 0.835036
</span></span><span class="line"><span class="cl">Layer 19. mean: 0.433965	std: 0.776518
</span></span><span class="line"><span class="cl">Layer 20. mean: 0.335705	std: 0.556218
</span></span></code></pre></div><p>‚ü∂ Accepted ‚úÖ</p>
<h2 id="3-another-approach">3. Another Approach</h2>
<p>Another technique was recently developed called <strong>batch normalization</strong>. We will take a look at it later.
P/s: A huge advantage of batch normalization is that it is more robust to bad initialization. In fact, using different values of weight scale does not produce much significant performance (and it&rsquo;s already good). Therefore, tuning hyperparams would be less painful.</p>
<h2 id="4-conclusion">4. Conclusion</h2>
<p>We have discussed <em>covariate shift</em> and its impact on training performance. With a proper scheme of initialization, the training could be less likely to get into this problem.
We also have a look at some examples to see the distribution of activations corresponds to different schemes of initialization.</p>
<p>Hope this post provides helpful visualization to help understand some problems of neural networks.</p>
<p>Reference:</p>
<ol>
<li><a href="http://cs231n.github.io/neural-networks-2">CS231n course notes</a></li>
<li><a href="http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf">Understanding the difficulty of training deep feedforward neural networks</a></li>
<li><a href="https://arxiv.org/pdf/1502.01852v1.pdf">Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</a></li>
</ol>
<p>Source code: <a href="https://github.com/trinhngocthuyen/teach-myself-ml/blob/master/experiments/weight_initialization.ipynb">here</a></p>

  <div class="entry-footer">
  
<div class="categories">
  <svg class='icon' viewbox='0 0 24 24' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' aria-hidden='true'>
  <path d="M22,19a2,2,0,0,1-2,2H4a2,2,0,0,1-2-2V5A2,2,0,0,1,4,3H9l2,3h9a2,2,0,0,1,2,2Z"/>
</svg>

  <a href="/categories/tech" class="category">tech</a>
</div>
  
<div class="tags">
  <svg class='icon' viewbox='0 0 24 24' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' aria-hidden='true'>
  <path d="M20.59,13.41l-7.17,7.17a2,2,0,0,1-2.83,0L2,12V2H12l8.59,8.59A2,2,0,0,1,20.59,13.41Z"/>
  <line x1="7" y1="7" x2="7" y2="7"/>
</svg>

  <a href="/tags/neural-networks" class="tag">neural-networks</a>
</div>
</div>
</article>
<nav id="toc">
<h4 class="toc-header">Table of Contents</h4>
<div><a href="#1-covariate-shift" toc_id="1-covariate-shift">1. Covariate Shift</a></div><div class="toc-section">

<div><a href="#11-what-is-covariate-shift" toc_id="11-what-is-covariate-shift">1.1. What is Covariate Shift?</a></div>
<div><a href="#12-what-does-it-look-like" toc_id="12-what-does-it-look-like">1.2 .What Does it Look Like?</a></div>
<div><a href="#13-impact-of-covariate-shift" toc_id="13-impact-of-covariate-shift">1.3. Impact of Covariate Shift?</a></div></div>
<div><a href="#2-param-initialization" toc_id="2-param-initialization">2. Param Initialization</a></div><div class="toc-section">

<div><a href="#21-setup" toc_id="21-setup">2.1. Setup</a></div>
<div><a href="#22-trivial-initialization" toc_id="22-trivial-initialization">2.2. Trivial Initialization</a></div>
<div><a href="#23-xavier-initialization-glorot-initialization" toc_id="23-xavier-initialization-glorot-initialization">2.3. Xavier Initialization (Glorot Initialization)</a></div></div>
<div><a href="#3-another-approach" toc_id="3-another-approach">3. Another Approach</a></div>
<div><a href="#4-conclusion" toc_id="4-conclusion">4. Conclusion</a></div></nav><script src="/scripts/toc.js"></script>
</div>

<nav class='entry-nav'><div class='prev-entry'>
    <a href='/posts/tech/a-dive-into-hog/'>A dive into Histogram of Oriented Gradients (HOG)</a>
  </div><div class='next-entry'>
    <a href='/posts/tech/a-misuse-of-expectation/'>A misuse of Expectation</a>
  </div></nav>

<section id='comments' class='comments'>
  <div class="container">
    
<div id="fb-root"></div>
<script async defer crossorigin="anonymous" src="https://connect.facebook.net/en_US/sdk.js#xfbml=1&version=v17.0"></script>
<div class="share">
  <div class="fb-like" data-href="https://trinhngocthuyen.com/posts/tech/weight-initialization/" data-width="" data-layout="standard" data-action="like" data-share="true"></div>
</div>
  </div>
  <div class="container">
    <script type="text/javascript" src="https://cdnjs.buymeacoffee.com/1.0.0/button.prod.min.js" data-name="bmc-button" data-slug="trinhngocthuyen" data-color="#FFDD00" data-emoji=""  data-font="Cookie" data-text="Buy me a coffee" data-outline-color="#000000" data-font-color="#000000" data-coffee-color="#ffffff" ></script>
  </div>
</section>

</main><script type='text/x-mathjax-config'>
  MathJax.Hub.Config({"HTML-CSS":{"availableFonts":["TeX"],"linebreaks":{"automatic":true},"scale":90},"TeX":{"Macros":{"dim":["{\\color{gray}#1}",1],"txt":["\\hspace{3pt}\\text{#1}\\hspace{3pt}",1]},"TagSide":"left","extensions":["color.js"]},"extensions":["tex2jax.js","TeX/AMSmath.js","TeX/AMSsymbols.js"],"jax":["input/TeX","output/HTML-CSS"],"tex2jax":{"displayMath":[["$$","$$"]],"inlineMath":[["$","$"]],"processEnvironments":true,"processEscapes":true}})
</script>

<script type='text/javascript' async src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_HTML'></script></body>
</html>