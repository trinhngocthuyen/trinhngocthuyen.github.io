<!DOCTYPE html>
<html lang="en" class="js csstransforms3d">
  <head>
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width, initial-scale=1.0">
      <meta property="og:title" content="Weight initialization - impact on layer distribution">
      <title>Thuyen&#39;s corner</title>
      <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.8/styles/default.min.css">
      <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.3/css/all.css" integrity="sha384-UHRtZLI+pbxtHCWp1t77Bi1L4ZtiqrqD80Kn4Z8NTSRyMA2Fd33n5dQ8lWUE00s/" crossorigin="anonymous">
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.8/highlight.min.js"></script>
      
      <link rel="stylesheet" href="https://trinhngocthuyen.github.io/sass/main.7c9da60ff2d0ede050060cc37446d405a6ebc67b1b72688fd9329eba535d7f72.css">

      
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-69597239-3', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>

  </head>
  <body class="" data-url="/posts/tech/weight-initialization/">
    <nav role="navigation">
  <header role="banner">
    <h1 id="logo">
      <a href="/">Thuyen&#39;s corner</a>
    </h1>
  </header>
  <div id="menu-toggle" class="menu-toggle">
    <span class='open'><svg class='icon' viewbox='0 0 24 24' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' aria-hidden='true'>
  
  <line x1="3" y1="12" x2="21" y2="12" />
  <line x1="3" y1="6" x2="21" y2="6" />
  <line x1="3" y1="18" x2="21" y2="18" />
  
</svg>
</span>
    <span class='close'><svg class='icon' viewbox='0 0 24 24' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' aria-hidden='true'>
  
  <line x1="18" y1="6" x2="6" y2="18" />
  <line x1="6" y1="6" x2="18" y2="18" />
  
</svg>
</span>
  </div>
  <ul>
    
    <li><a href="/posts/tech">Tech</a></li>
    
    <li><a href="/posts/misc">Misc (Tạp bút)</a></li>
    
    <li><a href="/about">About</a></li>
    
  </ul>
</nav>

<script src="/scripts/menu.js"></script>
    <main role="main">
<article class="page">
  <header>
  
  <h1>Weight initialization - impact on layer distribution</h1>
  
  
  <div class="metadata">
  <span>2017, Jul 25</span>
</div>
  
</header>
  <p>In this post, I will carry out some experiments to demonstrate the impact of weight initialization on the distribution of activations on each layer in neural network, especially the very last layers. This was mentioned by Andrej in <a href="https://www.youtube.com/watch?v=hd_KFJ5ktUc&amp;index=6&amp;list=PLkt2uSq6rBVctENoVBg1TpCC7OQi31AlC">cs231n/lec6</a> as a motivation paving the way for batch normalization.</p>
<p>I will try to keep things simple with intuition. Mathematics will not be richly convered.</p>
<h3 id="1-covariate-shift">1. Covariate shift</h3>
<h4 id="11-what-is-covariate-shift">1.1. What is Covariate shift?</h4>
<p><em>Covariate shift</em> refers to changes in the distribution of input variables. This shift is usually addressed as a problem causing poor performance in training neural networks.</p>
<p>Note: In the context of neural networks, a layer takes the activations of the previous layer as its input. Therefore, it is also equivalent if we investigate the distribution of activations of a layer.</p>
<h4 id="12-what-does-it-look-like">1.2 .What does it look like?</h4>
<p>Consider a neural networks with 20 fully connected layers.</p>
<ul>
<li><em>Input shape</em>: 49000x3072 (reshape from 49000 CIFAR images of size 32x32) and was whitened (zero-centered &amp; normalized).</li>
<li><em>Hidden layers</em>: 19 layers of shape (100, 100)</li>
<li><em>Activation function</em>: ReLU.</li>
<li><em>Biases</em>: all zeros.</li>
<li><em>Weights</em>: normal distribution, scaled by factor 0.1.</li>
</ul>
<p>The distributions of layer ouputs are illustrated in the figure below. We can apparently see that the variance constantly decreases in deeper layers.</p>
<img src="/images/notebooks/weight_initialization_5_0.png" width="600"/>
<figcaption><em>Fig 1.</em> Distribution of activations in layer 1, 3, 5, 7.</figcaption>
<h4 id="13-impact-of-covariate-shift">1.3. Impact of Covariate shift?</h4>
<p>When training a network, if the activations fall in a narrow range like above, the gradient on those activations could be relatively small, leading to <strong>gradient saturation/vanish</strong>. Training gets stuck as a result. We sometimes face the same problem with dispersing distribution.</p>
<h3 id="2-param-initialization">2. Param initialization</h3>
<p>Param initialization is a primary cause of covariate shift. We will focus on weight initialization, and let biases be all zeros.</p>
<p>An initialization strategy is considered good if it maintains the same mean and variance (or standard deviation) throughout layers. Since the input data was whitened, we expect mean around zero (+/-1), std around 1 (+/-0.5).</p>
<ul>
<li>$\mu = 0.3, \sigma = 0.7$  ⟶ accepted ✅</li>
<li>$\mu = 0.3, \sigma = 0.003$ ⟶ NOT accepted ❌</li>
<li>$\mu = 15, \sigma = 320$ ⟶ NOT accepted ❌</li>
</ul>
<h4 id="21-setup">2.1. Setup</h4>
<p>Some setup code for experiments:</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">relu</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">X_flat</span><span class="p">,</span> <span class="n">hid_dims</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">biases</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">relu</span><span class="p">):</span>
    <span class="s2">&#34;&#34;&#34;
</span><span class="s2">    Feed forward.
</span><span class="s2">     - X_flat: shape: N x D, where D = d1 x ... x dk.
</span><span class="s2">     - hid_dims: 1D array.
</span><span class="s2">     - weights: array of weights: W1, W2, ...
</span><span class="s2">     - biases: array of biases: b1, b2, ...
</span><span class="s2">     - activation: activation on each layer. None if no activation.
</span><span class="s2">    &#34;&#34;&#34;</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">biases</span><span class="p">),</span> <span class="s1">&#39;weights and biases must have the same length&#39;</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">hid_dims</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">weights</span><span class="p">),</span> <span class="s1">&#39;len(weights) must equal len(hid_dims) + 1&#39;</span>

    <span class="n">all_layers</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">X_flat</span>
    <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">hid_dims</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">weights</span><span class="p">[</span><span class="n">l</span><span class="p">])</span> <span class="o">+</span> <span class="n">biases</span><span class="p">[</span><span class="n">l</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">activation</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">activation</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">all_layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="n">all_layers</span>
</code></pre></div><div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">visualize_distribution</span><span class="p">(</span><span class="n">all_activations</span><span class="p">):</span>
    <span class="n">n_layers</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">all_activations</span><span class="p">)</span>
    <span class="n">activations_mean</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">asscalar</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">activations</span><span class="p">))</span> <span class="k">for</span> <span class="n">activations</span> <span class="ow">in</span> <span class="n">all_activations</span><span class="p">]</span>
    <span class="n">activations_std</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">asscalar</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">activations</span><span class="p">))</span> <span class="k">for</span> <span class="n">activations</span> <span class="ow">in</span> <span class="n">all_activations</span><span class="p">]</span>

    <span class="n">xs</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_layers</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
    <span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">,</span> <span class="n">ax3</span><span class="p">,</span> <span class="n">ax4</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">activations_mean</span><span class="p">,</span> <span class="s1">&#39;-o&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;#0067a7&#39;</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;means by layer&#39;</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;layer&#39;</span><span class="p">)</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">activations_std</span><span class="p">,</span> <span class="s1">&#39;-o&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">)</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;std by layer&#39;</span><span class="p">)</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;layer&#39;</span><span class="p">)</span>

    <span class="n">ax3</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">all_activations</span><span class="p">[</span><span class="mi">14</span><span class="p">]</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">)</span>
    <span class="n">ax3</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Distribution of layer 15&#39;</span><span class="p">)</span>
    <span class="n">ax4</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">all_activations</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">)</span>
    <span class="n">ax4</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Distribution of the layer </span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">n_layers</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

    <span class="c1"># Print mean and std of last 5 layer</span>
    <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_layers</span><span class="p">)[</span><span class="o">-</span><span class="mi">5</span><span class="p">:]:</span>
        <span class="k">print</span><span class="p">(</span><span class="s1">&#39;Layer </span><span class="si">%2d</span><span class="s1">. mean: </span><span class="si">%f</span><span class="se">\t</span><span class="s1">std: </span><span class="si">%f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">l</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">activations_mean</span><span class="p">[</span><span class="n">l</span><span class="p">],</span> <span class="n">activations_std</span><span class="p">[</span><span class="n">l</span><span class="p">]))</span>

<span class="k">def</span> <span class="nf">initialize_params</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">hid_dims</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">,</span>
                      <span class="n">weight_initializer</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">,</span>
                      <span class="n">bias_initializer</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">):</span>
    <span class="n">dims</span> <span class="o">=</span> <span class="p">[</span><span class="n">in_dim</span><span class="p">]</span> <span class="o">+</span> <span class="n">hid_dims</span> <span class="o">+</span> <span class="p">[</span><span class="n">out_dim</span><span class="p">]</span>
    <span class="n">weights</span><span class="p">,</span> <span class="n">biases</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">hid_dims</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">weight_initializer</span><span class="p">(</span><span class="n">dims</span><span class="p">[</span><span class="n">l</span><span class="p">],</span> <span class="n">dims</span><span class="p">[</span><span class="n">l</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">bias_initializer</span><span class="p">(</span><span class="n">dims</span><span class="p">[</span><span class="n">l</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">weights</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
        <span class="n">biases</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">weights</span><span class="p">,</span> <span class="n">biases</span>

<span class="k">def</span> <span class="nf">examine_distribution</span><span class="p">(</span><span class="n">weight_initializer</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">relu</span><span class="p">,</span> <span class="n">n_layers</span><span class="o">=</span><span class="mi">20</span><span class="p">):</span>
    <span class="n">in_dim</span><span class="p">,</span> <span class="n">out_dim</span> <span class="o">=</span> <span class="mi">32</span><span class="o">*</span><span class="mi">32</span><span class="o">*</span><span class="mi">3</span><span class="p">,</span> <span class="mi">10</span>
    <span class="n">hid_dims</span> <span class="o">=</span> <span class="p">[</span><span class="mi">100</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_layers</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)]</span>

    <span class="n">weights</span><span class="p">,</span> <span class="n">biases</span> <span class="o">=</span> <span class="n">initialize_params</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">hid_dims</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">,</span>
        <span class="n">weight_initializer</span><span class="o">=</span><span class="n">weight_initializer</span><span class="p">)</span>

    <span class="n">_</span><span class="p">,</span> <span class="n">all_activations</span> <span class="o">=</span> <span class="n">forward</span><span class="p">(</span><span class="n">X_train_normalized</span><span class="p">,</span> <span class="n">hid_dims</span><span class="o">=</span><span class="n">hid_dims</span><span class="p">,</span> 
        <span class="n">weights</span><span class="o">=</span><span class="n">weights</span><span class="p">,</span> <span class="n">biases</span><span class="o">=</span><span class="n">biases</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">)</span>

    <span class="n">visualize_distribution</span><span class="p">(</span><span class="n">all_activations</span><span class="p">)</span>
</code></pre></div><p>Now, lets examine some initialization schemes&hellip;</p>
<h4 id="22-trival-initialization">2.2. Trival initialization</h4>
<p>The common way is using normal randomization with a scale. This scale should equal the standard deviation of the random numbers.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">trivial_initializer</span><span class="p">(</span><span class="n">d1</span><span class="p">,</span> <span class="n">d2</span><span class="p">,</span> <span class="n">weight_scale</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">weight_scale</span><span class="p">,</span> <span class="p">(</span><span class="n">d1</span><span class="p">,</span> <span class="n">d2</span><span class="p">))</span>

<span class="n">examine_distribution</span><span class="p">(</span><span class="n">trivial_initializer</span><span class="p">)</span>
</code></pre></div><img src="/images/notebooks/weight_initialization_6_0.png" width="600"/>
<figcaption><em>Fig 2.</em> mean and std by layer. Initialized by Gaussian $\mu=0, \sigma^2=0.1$.</figcaption>
<div class="highlight"><pre class="chroma"><code class="language-fallback" data-lang="fallback">Layer 16. mean: 0.009006    std: 0.016165
Layer 17. mean: 0.005849    std: 0.009699
Layer 18. mean: 0.004207    std: 0.007308
Layer 19. mean: 0.002917    std: 0.004961
Layer 20. mean: 0.005232    std: 0.006176
</code></pre></div><p>⟶ Not accepted ❌</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">examine_distribution</span><span class="p">(</span><span class="k">lambda</span> <span class="n">d1</span><span class="p">,</span> <span class="n">d2</span><span class="p">:</span> <span class="n">trivial_initializer</span><span class="p">(</span><span class="n">d1</span><span class="p">,</span> <span class="n">d2</span><span class="p">,</span> <span class="n">weight_scale</span><span class="o">=</span><span class="mf">0.2</span><span class="p">))</span>
</code></pre></div><img src="/images/notebooks/weight_initialization_7_0.png" width="600"/>
<figcaption><em>Fig 3.</em> mean and std by layer. Initialized by Gaussian $\mu=0, \sigma^2=0.2$.</figcaption>
<div class="highlight"><pre class="chroma"><code class="language-fallback" data-lang="fallback">Layer 16. mean: 825.965548	std: 1394.566496
Layer 17. mean: 1491.038728	std: 2237.786857
Layer 18. mean: 2149.680649	std: 3300.200497
Layer 19. mean: 3008.496905	std: 4961.155125
Layer 20. mean: 2845.114905	std: 4381.190124
</code></pre></div><p>⟶ Not accepted ❌</p>
<p>Notice that after changing weight scale from 0.1 to 0.2, how output values are distributed changed radically. Quite sensitive to weight scale, huh?</p>
<h4 id="23-xavier-initialization-glorot-initialization">2.3. Xavier initialization (Glorot initialization)</h4>
<p>This scheme is based on the observation that:</p>
<p>$$\begin{equation}
var(s) = var(\sum_i^N{w_i.x_i}) = &hellip; = N.var(w).var(x)
\end{equation}$$</p>
<p>In order to keep the same variance, we need: $var(w) = 1/N$. So, the weight scale should be $\sqrt{1/N}$</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">xavier_initializer_1</span><span class="p">(</span><span class="n">d1</span><span class="p">,</span> <span class="n">d2</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">d1</span><span class="p">,</span> <span class="n">d2</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="n">d1</span><span class="p">)</span>

<span class="n">examine_distribution</span><span class="p">(</span><span class="n">xavier_initializer_1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
</code></pre></div><img src="/images/notebooks/weight_initialization_8_0.png" width="600"/>
<figcaption><em>Fig 4.</em> mean and std by layer. Initialized using xavier initialization, with no activation.</figcaption>
<div class="highlight"><pre class="chroma"><code class="language-fallback" data-lang="fallback">Layer 16. mean: 0.000000	std: 0.742681
Layer 17. mean: -0.000000	std: 0.731260
Layer 18. mean: -0.000000	std: 0.718502
Layer 19. mean: 0.000000	std: 0.724088
Layer 20. mean: -0.000000	std: 0.740663
</code></pre></div><p>⟶ Accepted ✅</p>
<p>We can see that xavier initialization gives a really nice distribution: <em>gaussian shape, reasonable mean and std</em>. However, this initialization strategy seems not to work well with non-linear activation. If we use ReLU as the activation function, we got a poor distribution.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">examine_distribution</span><span class="p">(</span><span class="n">xavier_initializer_1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">relu</span><span class="p">)</span>
</code></pre></div><img src="/images/notebooks/weight_initialization_9_0.png" width="600"/>
<figcaption><em>Fig 5.</em> mean and std by layer. With relu activation, normal distribution, $var(w) = 1/N$.</figcaption>
<div class="highlight"><pre class="chroma"><code class="language-fallback" data-lang="fallback">Layer 16. mean: 0.002048	std: 0.003093
Layer 17. mean: 0.001338	std: 0.002246
Layer 18. mean: 0.000834	std: 0.001481
Layer 19. mean: 0.000434	std: 0.000776
Layer 20. mean: 0.000440	std: 0.000590
</code></pre></div><p>⟶ Not accepted ❌</p>
<p>A recent paper by <a href="https://arxiv.org/pdf/1502.01852v1.pdf">He et al</a> dove into the analysis with ReLU activation and finally reached the conclusion that the variance of weights should be $2/N$. This is often used in practice.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">xavier_initializer_2</span><span class="p">(</span><span class="n">d1</span><span class="p">,</span> <span class="n">d2</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">d1</span><span class="p">,</span> <span class="n">d2</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">2.0</span> <span class="o">/</span> <span class="n">d1</span><span class="p">)</span>

<span class="n">examine_distribution</span><span class="p">(</span><span class="n">xavier_initializer_2</span><span class="p">)</span>
</code></pre></div><img src="/images/notebooks/weight_initialization_10_0.png" width="600"/>
<figcaption><em>Fig 6.</em> mean and std by layer. With relu activation, normal distribution, $var(w) = 2/N$.</figcaption>
<div class="highlight"><pre class="chroma"><code class="language-fallback" data-lang="fallback">Layer 16. mean: 0.605864	std: 0.924794
Layer 17. mean: 0.569898	std: 0.860651
Layer 18. mean: 0.530719	std: 0.835036
Layer 19. mean: 0.433965	std: 0.776518
Layer 20. mean: 0.335705	std: 0.556218
</code></pre></div><p>⟶ Accepted ✅</p>
<h3 id="3-another-approaches">3. Another approaches</h3>
<p>Another technique was recently developed called <strong>batch normalization</strong>. We will take a look at it later.
P/s: A huge advantage of batch normalization is that it is more robust to bad initialization. In fact, using different values of weight scale does not produce much significant performance (and it&rsquo;s already good). Therefore, tuning hyperparams would be less painful.</p>
<h3 id="4-conclusion">4. Conclusion</h3>
<p>We have discussed <em>covariate shift</em> and its impact on training performance. With a proper scheme of initialization, the training could be less likely to get into this problem.
We also have a look at some examples to see the distribution of activations corresponds to different schemes of initialization.</p>
<p>Hope this post provides helpful visualization to help understand some problems of neural networks.</p>
<p>Reference:</p>
<ol>
<li><a href="http://cs231n.github.io/neural-networks-2">CS231n course notes</a></li>
<li><a href="http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf">Understanding the difficulty of training deep feedforward neural networks</a></li>
<li><a href="https://arxiv.org/pdf/1502.01852v1.pdf">Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</a></li>
</ol>
<p>Source code: <a href="https://github.com/trinhngocthuyen/teach-myself-ml/blob/master/experiments/weight_initialization.ipynb">here</a></p>

  <div class="entry-footer">
  
<div class="categories">
  <svg class='icon' viewbox='0 0 24 24' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' aria-hidden='true'>
  
  <path d="M22,19a2,2,0,0,1-2,2H4a2,2,0,0,1-2-2V5A2,2,0,0,1,4,3H9l2,3h9a2,2,0,0,1,2,2Z"/>
  
</svg>

  
  <a href="/categories/tech" class="category">tech</a>
  
</div>

  
<div class="tags">
  <svg class='icon' viewbox='0 0 24 24' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' aria-hidden='true'>
  
  <path d="M20.59,13.41l-7.17,7.17a2,2,0,0,1-2.83,0L2,12V2H12l8.59,8.59A2,2,0,0,1,20.59,13.41Z"/>
  <line x1="7" y1="7" x2="7" y2="7"/>
  
</svg>

  
  <a href="/tags/neural-networks" class="tag">neural-networks</a>
  
</div>

</div>
</article>


<nav class='entry-nav'><div class='prev-entry'>
    <a href='/posts/tech/a-dive-into-hog/'>A dive into Histogram of Oriented Gradients (HOG)</a>
  </div><div class='next-entry'>
    <a href='/posts/tech/a-misuse-of-expectation/'>A misuse of Expectation</a>
  </div></nav>


<section id='comments' class='comments'>
  
  
  <div class="container">
    <div id="fb-root"></div>
<script async defer crossorigin="anonymous" src="https://connect.facebook.net/en_US/sdk.js#xfbml=1&version=v5.0"></script>
<div class="share">
  <div class="fb-like" data-href="https://trinhngocthuyen.github.io/posts/tech/weight-initialization/" data-width="" data-layout="standard" data-action="like" data-share="true"></div>
</div>

  </div>
  
  
  <div class="container">
    
  </div>
</section>


</main><script type='text/x-mathjax-config'>
  MathJax.Hub.Config({"HTML-CSS":{"availableFonts":["TeX"],"linebreaks":{"automatic":true},"scale":90},"TeX":{"Macros":{"dim":["{\\color{gray}#1}",1],"txt":["\\hspace{3pt}\\text{#1}\\hspace{3pt}",1]},"TagSide":"left","extensions":["color.js"]},"extensions":["tex2jax.js","TeX/AMSmath.js","TeX/AMSsymbols.js"],"jax":["input/TeX","output/HTML-CSS"],"tex2jax":{"displayMath":[["$$","$$"]],"inlineMath":[["$","$"]],"processEnvironments":true,"processEscapes":true}})
</script>

<script type='text/javascript' async src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_HTML'></script></body>
</html>